{"cells":[{"cell_type":"markdown","metadata":{"id":"rYg34ygXl9KF"},"source":["# helpers.py"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1663619160842,"user":{"displayName":"C N","userId":"12515862087720570501"},"user_tz":-120},"id":"-nWjRtmhcvgy"},"outputs":[],"source":["#---------------------------------------------------------------------------------------------------#\n","# File name: helpers.py                                                                             #\n","# Autor: Chrissi2802                                                                                #\n","# Created on: 11.09.2022                                                                            #\n","#---------------------------------------------------------------------------------------------------#\n","# Exact description in the functions.\n","# This file provides auxiliary classes and functions for neural networks.\n","\n","\n","from datetime import datetime\n","import tensorflow as tf\n","\n","\n","class Program_runtime():\n","    \"\"\"Class for calculating the programme runtime and outputting it to the console.\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialisation of the class (constructor). Automatically saves the start time.\"\"\"\n","\n","        self.begin()\n","\n","    def begin(self):\n","        \"\"\"This method saves the start time.\"\"\"\n","\n","        self.__start = datetime.now()   # start time\n","\n","    def finish(self, print = True):\n","        \"\"\"This method saves the end time and calculates the runtime.\"\"\"\n","        # Input:\n","        # print; boolean, default false, the start time, end time and the runtime should be output to the console\n","        # Output:\n","        # self.__runtime; integer, returns the runtime\n","\n","        self.__end = datetime.now() # end time\n","        self.__runtime = self.__end - self.__start  # runtime\n","\n","        if (print == True):\n","            self.show()\n","\n","        return self.__runtime\n","\n","    def show(self):\n","        \"\"\"This method outputs start time, end time and the runtime on the console.\"\"\"\n","\n","        print()\n","        print(\"Start:\", self.__start.strftime(\"%Y-%m-%d %H:%M:%S\"))\n","        print(\"End:  \", self.__end.strftime(\"%Y-%m-%d %H:%M:%S\"))\n","        print(\"Program runtime:\", str(self.__runtime).split(\".\")[0])    # Cut off milliseconds\n","        print()\n","\n","\n","def hardware_config(device = \"GPU\"):\n","    \"\"\"This function configures the hardware.\"\"\"\n","    # Input:\n","    # device; string default GPU, which device to use, TPU or GPU\n","    # Output:\n","    # strategy; tensorflow MirroredStrategy\n","\n","    if (device == \"TPU\"):\n","        # TPU, use only if TPU is available\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","        strategy = tf.distribute.TPUStrategy(tpu)\n","    else:\n","        # GPU, if not available, CPU is automatically selected\n","        gpus = tf.config.list_logical_devices(\"GPU\")\n","        strategy = tf.distribute.MirroredStrategy(gpus)\n","\n","    return strategy\n","\n","\"\"\"\n","if (__name__ == \"__main__\"):\n","    \n","    # calculating the programme runtime\n","    Pr = Program_runtime()\n","    # Code here\n","    Pr.finish(print = True)\n","\n","    # configures the hardware\n","    strategy = hardware_config(\"GPU\")\n","\n","    with strategy.scope():\n","        pass\n","        # Code here \n","\"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"mmJnxIOUcvg1"},"source":["# dataset.py"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1663619160844,"user":{"displayName":"C N","userId":"12515862087720570501"},"user_tz":-120},"id":"UMlnEV4Ccvg2"},"outputs":[],"source":["#---------------------------------------------------------------------------------------------------#\n","# File name: dataset.py                                                                             #\n","# Autor: Chrissi2802                                                                                #\n","# Created on: 08.09.2022                                                                            #\n","#---------------------------------------------------------------------------------------------------#\n","# Google Brain - Ventilator Pressure Prediction (VPP)\n","# Exact description in the functions.\n","# This file provides the datasets and prepares the data.\n","\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import RobustScaler\n","from datetime import datetime\n","\n","\n","class VPP_Datasets():\n","    \"\"\"Class to design the Ventilator Pressure Prediction (VPP) Datasets.\"\"\"\n","\n","    def __init__(self, create_new = True, many_features = True):\n","        \"\"\"Initialisation of the class (constructor). It prepares the data to be used for training and testing.\"\"\"\n","        # Input:\n","        # create_new; boolean default True, create new data or load old data\n","        # many_features; boolean default True, many features should be added\n","\n","        print(\"Prepare the dataset for training and testing ...\")\n","\n","        self.path = \"./Dataset/\"    \n","        self.submission = pd.read_csv(self.path + \"sample_submission.csv\")\n","        self.create_new = create_new\n","        self.many_features = many_features\n","\n","        if (self.create_new == True):\n","            self.dataset_train = pd.read_csv(self.path + \"train.csv\")\n","            self.dataset_test = pd.read_csv(self.path + \"test.csv\")\n","            self.__feature_engineering()\n","\n","            if (self.many_features == False):\n","                self.__visualisation()\n","\n","            self.__clean_and_split()\n","            self.__normalize()\n","            self.__reshape()\n","        else:\n","            self.load_dataset_numpy()\n","\n","        print(\"Preparation of the data completed!\")\n","\n","    def __feature_engineering(self):\n","        \"\"\"This method extends and changes the features.\"\"\"\n","\n","        if (self.many_features == True):\n","            # many features from the internet\n","            self.drop_columns = [\"id\", \"breath_id\", \"one\", \"count\", \"breath_id_lag\", \"breath_id_lag2\", \n","                                 \"breath_id_lagsame\", \"breath_id_lag2same\", \"pressure\"]\n","            self.dataset_train = self.__add_many_features(self.dataset_train)\n","            self.dataset_test = self.__add_many_features(self.dataset_test)\n","        else:\n","            # few features from the internet\n","            self.drop_columns = [\"id\", \"breath_id\", \"pressure\"]\n","            self.__add_features(self.dataset_train)\n","            self.__add_features(self.dataset_test)\n","\n","    def __add_features(self, dataset):\n","        \"\"\"This method adds a few features from the internet to a DataFrame.\"\"\"\n","        # Code from: https://www.kaggle.com/competitions/ventilator-pressure-prediction/discussion/273974\n","        # Input:\n","        # dataset; DataFrame\n","\n","        # add a new feature which is the cumulative sum of the u_in feature\n","        dataset[\"u_in_cumsum\"] = (dataset[\"u_in\"]).groupby(dataset[\"breath_id\"]).cumsum()\n","\n","    def __visualisation(self):\n","        \"\"\"This method visualises the data.\"\"\"\n","\n","        self.dataset_train.head(1000).plot(subplots = True, sharex = True, title = \"Example data\", figsize = (16, 9), layout = (5, 2))\n","        plt.savefig(\"Example_data.png\")\n","        plt.show()\n","\n","    def __clean_and_split(self):\n","        \"\"\"This method splits the data from the labels and deletes unusable features.\"\"\"\n","\n","        self.targets = self.dataset_train[\"pressure\"].to_numpy()\n","        self.dataset_train = self.dataset_train.drop(self.drop_columns, axis = 1)\n","\n","        self.drop_columns.pop() # Delete last element, targets\n","        self.dataset_test = self.dataset_test.drop(self.drop_columns, axis = 1)\n","\n","    def __normalize(self):\n","        \"\"\"This method scales / normalises the features.\"\"\"\n","\n","        scaler = RobustScaler()\n","\n","        self.dataset_train = scaler.fit_transform(self.dataset_train)\n","        self.dataset_test = scaler.transform(self.dataset_test)\n","\n","    def __reshape(self):\n","        \"\"\"This method reshapes the data as it has a time dependency.\"\"\"\n","\n","        # After 80 steps in the timestamp, it starts again at 0\n","        self.dataset_train = self.dataset_train.reshape(-1, 80, self.dataset_train.shape[-1])\n","        self.dataset_test = self.dataset_test.reshape(-1, 80, self.dataset_test.shape[-1])\n","        self.targets = self.targets.reshape(-1, 80)\n","\n","    def get_datasets(self):\n","        \"\"\"This method returns the training data, labels and test data.\"\"\"\n","        # Output:\n","        # self.dataset_train, self.targets, self.dataset_test; numpy arrays\n","\n","        return self.dataset_train, self.targets, self.dataset_test\n","\n","    def get_path(self):\n","        \"\"\"This method returns the path.\"\"\"\n","        # Output:\n","        # self.path; string\n","\n","        return self.path\n","\n","    def save_datasets_numpy(self):\n","        \"\"\"This method saves the data arrays to a binary file in NumPy .npy format.\"\"\"\n","        \n","        np.save(self.path + \"train.npy\", self.dataset_train)\n","        np.save(self.path + \"target.npy\", self.targets)\n","        np.save(self.path + \"test.npy\", self.dataset_test)\n","        print(\"Data saved as NumPy files!\")\n","\n","    def load_dataset_numpy(self):\n","        \"\"\"This method loads arrays from .npy files.\"\"\"\n","\n","        self.dataset_train = np.load(self.path + \"train.npy\")\n","        self.targets = np.load(self.path + \"target.npy\")\n","        self.dataset_test = np.load(self.path + \"test.npy\")\n","        print(\"Data loaded from NumPy files!\")\n","\n","    def write_submissions_mean(self, test_predictions):\n","        \"\"\"This method writes the predictions from the cross-validation (the average of all predictions) into a csv file.\"\"\"\n","        # Input:\n","        # test_predictions; numpy array\n","\n","        # Every single fold is used. The average value is calculated and saved.\n","        self.submission[\"pressure\"] = test_predictions.mean(axis = 1)    # Mean of row\n","        now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","        self.submission.to_csv(self.path + now + \"_mean_submission.csv\", index = False)\n","\n","    def __add_many_features(self, df):\n","        \"\"\"This method adds many features from the internet to a DataFrame.\"\"\"\n","        # Code from: https://www.kaggle.com/code/mohitsahal/lstm-plus-gru\n","        # Input:\n","        # df; DataFrame\n","        # Output:\n","        # df; DataFrame\n","\n","        # Step 1\n","        df['cross']= df['u_in'] * df['u_out']\n","        df['cross2']= df['time_step'] * df['u_out']\n","        df['area'] = df['time_step'] * df['u_in']\n","        df['area'] = df.groupby('breath_id')['area'].cumsum()\n","        df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n","        df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n","        #print(\"Step-1...Completed\")\n","        \n","        # Step 2\n","        df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n","        df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n","        df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n","        df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n","        df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n","        df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n","        df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n","        df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n","        df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n","        df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n","        df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n","        df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n","        df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n","        df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n","        df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n","        df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n","        df = df.fillna(0)\n","        #print(\"Step-2...Completed\")\n","        \n","        # Step 3\n","        df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n","        df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n","        df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n","        df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n","        #print(\"Step-3...Completed\")\n","        \n","        # Step 4\n","        df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n","        df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n","        df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n","        df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n","        df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n","        df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n","        df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n","        df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n","        #print(\"Step-4...Completed\")\n","        \n","        # Step 5\n","        df['one'] = 1\n","        df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n","        df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n","        df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n","        df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n","        df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n","        df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n","        df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n","        df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n","        df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n","        df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n","        #print(\"Step-5...Completed\")\n","        \n","        # Step 6\n","        df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n","        # This feature leads to errors with the Pandas version used. Therefore, it is not used.\n","        #df['ewm_u_in_mean'] = (df\\\n","        #                    .groupby('breath_id')['u_in']\\\n","        #                    .ewm(halflife=9)\\\n","        #                    .mean()\\\n","        #                    .reset_index(level=0,drop=True))\n","        df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n","                                                                .groupby('breath_id')['u_in']\\\n","                                                                .rolling(window=15,min_periods=1)\\\n","                                                                .agg({\"15_in_sum\":\"sum\",\n","                                                                        \"15_in_min\":\"min\",\n","                                                                        \"15_in_max\":\"max\",\n","                                                                        \"15_in_mean\":\"mean\"})\\\n","                                                                .reset_index(level=0,drop=True))\n","        #print(\"Step-6...Completed\")\n","        \n","        # Step 7\n","        df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n","        df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n","        df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n","        df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n","        #print(\"Step-7...Completed\")\n","        \n","        # Step 8\n","        df['R'] = df['R'].astype(str)\n","        df['C'] = df['C'].astype(str)\n","        df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n","        df = pd.get_dummies(df)\n","        #print(\"Step-8...Completed\")\n","        \n","        return df\n","        \n","\"\"\"\n","if (__name__ == \"__main__\"):\n","    \n","    CVPP_Datasets = VPP_Datasets(create_new = True, many_features = False)\n","    train, target, test = CVPP_Datasets.get_datasets()\n","\n","    print(train.shape, target.shape, test.shape)\n","    #CVPP_Datasets.save_datasets_numpy()\n","    #CVPP_Datasets.write_submissions_mean(np.ones((4024000, 2)))\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"q6j0yP0Ucvg3"},"source":["# models.py"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1663619161162,"user":{"displayName":"C N","userId":"12515862087720570501"},"user_tz":-120},"id":"j66rKPOJcvg4"},"outputs":[],"source":["#---------------------------------------------------------------------------------------------------#\n","# File name: models.py                                                                              #\n","# Autor: Chrissi2802                                                                                #\n","# Created on: 08.09.2022                                                                            #\n","#---------------------------------------------------------------------------------------------------#\n","# Google Brain - Ventilator Pressure Prediction (VPP)\n","# Exact description in the functions.\n","# This file provides the models.\n","\n","\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.layers as layer\n","\n","\n","def gru_net(data):\n","    \"\"\"This function creates a GRU model in TensorFlow.\"\"\"\n","    # Input:\n","    # data; NumPy array, data fed into the model, here only relevant to find out the input shape\n","    # Output:\n","    # model; TensorFlow / Keras model, model for training and testing\n","\n","    x_input = layer.Input(shape = (data.shape[-2:]))\n","\n","    x = layer.Bidirectional(layer.GRU(256, return_sequences = True))(x_input)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Bidirectional(layer.GRU(128, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","\n","    x = layer.Dense(128, activation = \"relu\")(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Dropout(0.5)(x)\n","    x_output = layer.Dense(1)(x)\n","\n","    model = Model(inputs = x_input, outputs = x_output, name = \"GRU_NET\")\n","\n","    return model\n","\n","\n","def gru_net_big(data):\n","    \"\"\"This function creates a big GRU model in TensorFlow.\"\"\"\n","    # Input:\n","    # data; NumPy array, data fed into the model, here only relevant to find out the input shape\n","    # Output:\n","    # model; TensorFlow / Keras model, model for training and testing\n","\n","    x_input = layer.Input(shape = (data.shape[-2:]))\n","\n","    x = layer.Bidirectional(layer.GRU(1024, return_sequences = True))(x_input)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Bidirectional(layer.GRU(512, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","\n","    x = layer.Bidirectional(layer.GRU(256, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Bidirectional(layer.GRU(128, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","\n","    x = layer.Dense(128, activation = \"relu\")(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Dropout(0.5)(x)\n","    x_output = layer.Dense(1)(x)\n","\n","    model = Model(inputs = x_input, outputs = x_output, name = \"GRU_NET_BIG\")\n","\n","    return model\n","\n","\n","def lstm_net_big(data):\n","    \"\"\"This function creates a big LSTM model in TensorFlow.\"\"\"\n","    # Input:\n","    # data; NumPy array, data fed into the model, here only relevant to find out the input shape\n","    # Output:\n","    # model; TensorFlow / Keras model, model for training and testing\n","\n","    x_input = layer.Input(shape = (data.shape[-2:]))\n","\n","    x = layer.Bidirectional(layer.LSTM(1024, return_sequences = True))(x_input)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Bidirectional(layer.LSTM(512, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","\n","    x = layer.Bidirectional(layer.LSTM(256, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Bidirectional(layer.LSTM(128, return_sequences = True))(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","\n","    x = layer.Dense(128, activation = \"relu\")(x)\n","    x = layer.BatchNormalization(axis = -1)(x)\n","    x = layer.Dropout(0.5)(x)\n","    x_output = layer.Dense(1)(x)\n","\n","    model = Model(inputs = x_input, outputs = x_output, name = \"LSTM_NET_BIG\")\n","\n","    return model\n","\n","\"\"\"    \n","if (__name__ == \"__main__\"):\n","\n","    import dataset\n","\n","    # Dataset\n","    CVPP_Datasets = dataset.VPP_Datasets(create_new = False, many_features = True)\n","    train, target, test = CVPP_Datasets.get_datasets()\n","\n","    gru_small = gru_net(train)\n","    gru_big = gru_net_big(train)\n","    lstm_big = lstm_net_big(train)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"ci-hbY8_cvg5"},"source":["# train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnmoXpa3cvg5","outputId":"2746812d-71c4-4d38-9c89-e9346f8f900f"},"outputs":[],"source":["#---------------------------------------------------------------------------------------------------#\n","# File name: train.py                                                                               #\n","# Autor: Chrissi2802                                                                                #\n","# Created on: 08.09.2022                                                                            #\n","#---------------------------------------------------------------------------------------------------#\n","# Google Brain - Ventilator Pressure Prediction (VPP)\n","# Exact description in the functions.\n","# This file provides functions for training and testing.\n","\n","\n","#import dataset, models, helpers\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import KFold\n","import numpy as np\n","\n","\n","def plot_loss_and_error(train_losses, train_error, test_losses = [], test_error = [], fold = \"\"):\n","    \"\"\"This function plots the loss and error for training and, if available, for validation.\"\"\"\n","    # Input:\n","    # train_losses; list, Loss during training for each epoch\n","    # train_error; list, Error during training for each epoch\n","    # test_losses; list default [], Loss during validation for each epoch\n","    # test_error; list default [], Error during validation for each epoch\n","    # fold; string default \"\", Fold number\n","\n","    fig, ax1 = plt.subplots()\n","    epochs = len(train_losses)\n","    xaxis = list(range(1, epochs + 1))\n","\n","    # Training\n","    # Loss\n","    trl = ax1.plot(xaxis, train_losses, label = \"Training Loss\", color = \"red\")\n","    ax1.set_xlabel(\"Epochs\")\n","    ax1.set_ylabel(\"Loss\")\n","\n","    # Error\n","    ax2 = ax1.twinx()\n","    tra = ax2.plot(xaxis, train_error, label = \"Training Error\", color = \"fuchsia\")\n","    ax2.set_ylabel(\"Mean absolute percentage error in %\")\n","    ax2.set_ylim(0.0, 200.0)\n","    lns = trl + tra # Labels\n","\n","    # Test\n","    if ((test_losses != []) and (test_error != [])):\n","        # Loss\n","        tel = ax1.plot(xaxis, test_losses, label = \"Validation Loss\", color = \"lime\")\n","\n","        # Error\n","        tea = ax2.plot(xaxis, test_error, label = \"Validation Error\", color = \"blue\")\n","\n","        lns = trl + tel + tra + tea    # Labels\n","\n","    labs = [l.get_label() for l in lns]\n","    ax1.legend(lns, labs)\n","    plt.title(\"Loss and Error Fold \" + fold)\n","    fig.savefig(\"Loss_and_Error_Fold_\" + fold + \".png\")\n","    plt.show()\n","\n","\n","def train_vpp():\n","    \"\"\"This function performs the training and testing for the Ventilator Pressure Prediction (VPP) dataset.\"\"\"\n","\n","    # Hyperparameter\n","    epochs = 2 #500    # For testing 2\n","    batch_size = 1024\n","    verbose = 1\n","    \n","    # Hardware config\n","    strategy = hardware_config(\"GPU\") #helpers.hardware_config(\"GPU\")\n","\n","    # Disable AutoShard\n","    options = tf.data.Options()\n","    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n","\n","    with strategy.scope():\n","\n","        # Dataset\n","        CVPP_Datasets = VPP_Datasets(create_new = True, many_features = True) #dataset.VPP_Datasets(create_new = True, many_features = True)\n","        train, target, test = CVPP_Datasets.get_datasets()\n","\n","        path = CVPP_Datasets.get_path()\n","        path_models = path.rstrip(\"Dataset/\") + \"/Models/\"\n","\n","        # Crossvalidation\n","        k_fold = KFold(n_splits = 5, shuffle = True)    # For testing 2\n","        \n","        # Numpy array for the predictions\n","        test_predictions = np.empty([test.shape[0] * test.shape[1], k_fold.n_splits])\n","\n","        # Perform the crossvalidation\n","        for fold, (train_index, test_index) in enumerate(k_fold.split(train, target)):\n","\n","            print(\"Fold:\", fold)\n","\n","            # Data for this fold\n","            x_train, x_valid = train[train_index], train[test_index]\n","            y_train, y_valid = target[train_index], target[test_index]\n","\n","            # Wrap data in Dataset objects\n","            train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size).with_options(options)\n","            valid_data = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size).with_options(options)\n","            test_data = tf.data.Dataset.from_tensor_slices((test)).batch(batch_size).with_options(options)\n","\n","            # Model, choose one\n","            #model = gru_net(train) #models.gru_net(train)\n","            model = gru_net_big(train) #models.gru_net_big(train)\n","            #model = lstm_net_big(train) #models.lstm_net_big(train)\n","\n","            print(model.summary())\n","\n","            model.compile(optimizer = \"adam\", loss = \"mae\", metrics = [tf.keras.metrics.MeanAbsolutePercentageError()])\n","\n","            learning_rate = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 10, verbose = verbose)\n","            early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 60, verbose = verbose, mode = \"min\", \n","                                           restore_best_weights = True)\n","            model_checkpoint = ModelCheckpoint(path_models + model.name + str(fold) + \".hdf5\", monitor = \"val_loss\", verbose = verbose, save_best_only = True, \n","                                               mode = \"auto\", save_freq = \"epoch\")\n","\n","            # Training\n","            history = model.fit(train_data, \n","                                validation_data = valid_data, \n","                                epochs = epochs,\n","                                verbose = 2,    # for debugging verbose\n","                                batch_size = batch_size, \n","                                callbacks = [learning_rate, early_stopping, model_checkpoint])\n","\n","            # Plot training and testing curves\n","            plot_loss_and_error(history.history[\"loss\"], history.history[\"mean_absolute_percentage_error\"], \n","                                history.history[\"val_loss\"], history.history[\"val_mean_absolute_percentage_error\"], str(fold))\n","\n","            # Save predictions \n","            test_predictions[:, fold] = model.predict(test_data, batch_size = batch_size).squeeze().reshape(-1, 1).squeeze()\n","            print()\n","\n","        # Save submissions\n","        CVPP_Datasets.write_submissions_mean(test_predictions)\n","        print(\"Training, validation and testing completed!\")\n","\n","\n","if (__name__ == \"__main__\"):\n","\n","    Pr = Program_runtime() #helpers.Program_runtime()\n","\n","    train_vpp()\n","    \n","    Pr.finish(print = True)\n"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"}}},"nbformat":4,"nbformat_minor":0}
